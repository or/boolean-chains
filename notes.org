* Notes
** Optimizations
*** Unrolling the main loop into macros
- tested with N=12, L=18
- for a chunk that contains a solution: ~-c 8~ (solution is in ~-c 8 31~)
- the unrolled version is a little faster, 19.1s (e0354c1) vs 20.3s (65b1df3 +
  N/L adjustment)
- I've tried profiling and optimizing both (the new version requires a change
  for the chunks: ~goto loop_4~ for ~-c 8~ and ~goto loop_5~ for ~-c 8 31~):
  #+begin_quote
make target/full-search-profile
target/full-search-profile -c <chunk>
xcrun llvm-profdata merge -output=default.profdata default.profraw
make target/full-search-optimized
# for -c 8
time target/full-search-optimized -c 8
# for -c 8 31
hyperfine --warmup 5 'target/full-search -c 8 31'
  #+end_quote
- results:
| version | chunk | without profiling | with profiling | better than without | better than rolled |
|---------+-------+-------------------+----------------+---------------------+--------------------|
| 65b1df3 | 8     | 20.028 s          | 19.865 s       |               0.82% |                    |
| 65b1df3 | 8 31  | 400.0 ms          | 399.5 ms       |               0.13% |                    |
| e0354c1 | 8     | 19.106 s          | 18.714 s       |                2.1% |               4.8% |
| e0354c1 | 8 31  | 380.0 ms          | 374.4 ms       |                1.5% |               5.3% |
*** Using ~uint8~ instead of ~uint32~ for ~seen~ and ~target_lookup~
- should be more cache friendly, appears to improve performance by ~1-2% for N=12
- potentially has a slightly bigger impact for larger N, when the size of those
  arrays grows
